{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models (LLM)\n",
    "In this tutorial, we'll learn how to use Large Language Models to generate desired outputs. We will use Llama-2-7B Model in this tutorial. Different Use Cases are implemented using LLM\n",
    "\n",
    "## Step # 01\n",
    "- Check your GPU-memory usage through nvidia-smi. Clean up your memory to implement this project. \n",
    "- Install the necessary libraries required to run this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing Necessary Libraries\n",
    "%pip install torch==2.1.1                       #Deep Learning Framework to train, inference models\n",
    "%pip install transformers==4.35.2               #Hugging-Face library: to load, train, fine-tune LLMS\n",
    "%pip install accelerate==0.25.0                 #Optimized training & Inference across multiple GPUs, TPUs and CPUs\n",
    "%pip install bitsandbytes==0.41.3               #for 4-bit and 8-bit quantization to reduce memory usage\n",
    "%pip install huggingface_hub                    #to login hugging-face account\n",
    "%pip install ipywidgets                         #to create interactive widgets in JupiterNotebook\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step # 02\n",
    "Login your Hugging-Face account to download and use Llama-2-7B model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Login your HuggingFace account\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step # 03\n",
    "Load Llama-2-7B model from HuggingFace. Load both model and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load LLM tokenizer form HuggingFace\n",
    "from transformers import AutoTokenizer\n",
    "Model_name=\"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the Input Prompt\n",
    "text=\"The most important person in Deep Learning is\"\n",
    "encoding=tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids= encoding.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Llama-2-7B Base Model\n",
    "#AutoModelForCausalLM: To Load Causal Language Model from HuggingFace\n",
    "#GenerationConfig: Stores Configuration Setting for text generation\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(Model_name,device_map=\"auto\",torch_dtype=torch.float16)  #uses fp16 to save memory and speedup inference\n",
    "\n",
    "generation_config=GenerationConfig.from_pretrained(Model_name)\n",
    "generation_config.max_new_tokens = 128             # max no. of tokens a model can generate\n",
    "generation_config.repetition_penalty =  1.18       # prevents repetitive text with a repetition penalty to encourage diverse outputs. Values >1 reduce repetition (1.18 is a moderate penalty).\n",
    "generation_config.temperature = 0.0000001          # to control randomness in output. Lower value(~0): more deterministic. Higher value(>1): more randomness/creative in response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step # 04\n",
    "Generate simple response from the Loaded Llama-2-7B model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"         #Disabling parallelism to avoid deadlocks\n",
    "#Generate Simple Prediction from LLM\n",
    "#Load encoding to Model's Device\n",
    "encoding=encoding.to(model.device)\n",
    "\n",
    "!time  #measuring execution time\n",
    "with torch.no_grad():   #disabling gradient calculation\n",
    "    output=model.generate(     #generating text from LLM\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert output to human readable string\n",
    "output_text=tokenizer.decode(output[0],skip_special_tokens=True)  #skip special tokens like <eos>, <pad>, <s> etc    \n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 05: Instruction Tuned (Chat) Model\n",
    "As you can see the generated output is not our desired response. This response needs to be more accurate and effective. For this purpose, we'll do Instruction Tuning on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "model_name=\"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "#Load base Model\n",
    "model= AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",torch_dtype=torch.float16)\n",
    "\n",
    "generation_config=GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens= 512\n",
    "generation_config.repetition_penalty=1.18\n",
    "generation_config.temperature=0.0000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 07: Text Generation Pipeline\n",
    "Now, we'll create text generation pipeline to get our desired responses in real-time\n",
    "- TextStreamer: streams model output token by token, allowing real-time text generation (mostly used for real-time applications like ChatBots, live AI interactions)\n",
    "- pipeline: High-level API to setup text-generation models easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, pipeline\n",
    "#creating a text streamer: generates text in real-time rather than waiting for full output\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#Text-Generation Pipeline\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    "    num_return_sequences=1,                 #return only 1 resp\n",
    "    streamer=streamer                       #Streams the generated text live instead of returning it all at once.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=llm(\"Who is the most important person in Deep Learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 08: Prompt Format\n",
    "We will prepare a structured chat prompt for Llama-2-7B LLM using HuggingFace tokenizer.apply_chat_template() function. \n",
    "- The model will be instructed to always reply using Ludacris-style slang (i.e. energetic, street slang, hip hop tone)\n",
    "- This will control AI behvaior making it sound less formal and more like the rapper Ludacris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system prompt: Instruction to AI\n",
    "system_prompt=\"Act and always reply using slang that Ludacris uses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Chat History --> Message List\n",
    "messages = [{\"role\":\"system\", \"content\": system_prompt},\n",
    "            {\"role\":\"user\", \"content\":\"Who is the most important person in AI?\"},]   \n",
    "\n",
    "prompt=tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True) #Tokenize=False:Returns a readable text string instead of tokenized data\n",
    "print(prompt)                                                                            #generation_prompt=True:Ensures the model knows when to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 09: Content Generation\n",
    "Now, we'll generate content using prompt and system-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def predict(prompt: str, system_prompt: Optional[str] = None):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ]\n",
    "    if system_prompt:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "You're an expert WeightLoss Coach with 20+ years of experince with training people \n",
    "with all types of DietingPlans and Weight Loss exercises\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Outline 3 most important tips for quick weight loss\n",
    "\"\"\".strip()\n",
    "\n",
    "output=predict(prompt,system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You're slavic connoisseur. You love everything slavic and understand\n",
    "that it is superior (jokingly) to anything else.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the most iconic dish that is prepared by slavic grandmothers?\n",
    "\"\"\".strip()\n",
    "\n",
    "output = predict(prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 10: Coding\n",
    "Now, we'll ask LLM to do some coding using prompt and system-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time\n",
    "system_prompt = \"\"\"\n",
    "You're an experienced Python developer that writes efficient and readable code.\n",
    "You always strive to use built-in libraries.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a function that calculates the square sum of two numbers and divide it by 42\n",
    "\"\"\".strip()\n",
    "\n",
    "output = predict(prompt, system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time \n",
    "prompt=\"\"\"\n",
    "Write a function that fetched the daily prices of Tesla stock for the last week\n",
    "\"\"\".strip()\n",
    "\n",
    "output=predict(prompt,system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step # 11: Analyze Tweets\n",
    "Now, we'll ask LLM to do sentiment analysis of few tweets using prompt and system-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt=\"\"\"\n",
    "You're expert social media analyst. When analyzing text, you always take into \n",
    "account the content and put heavy importance on the author\n",
    "\"\"\"\n",
    "\n",
    "!time\n",
    "tweet=\"\"\"\n",
    "I hope that even my worst critics remain on Twiteer,\n",
    "because that is what free speech means\n",
    "-Elon Musk\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "What is the meaning of this tweet? Do sentiment analysis. \n",
    "Rewrite it in words of Putin.\n",
    "```\n",
    "{tweet}\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "output=predict(prompt, system_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
